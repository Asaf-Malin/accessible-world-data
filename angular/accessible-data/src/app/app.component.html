<pre class="about">
  Hi there,
  Welcome to accessible data project!
  It is yet another attempt to "organize the world information".

  One might ask, what is the point to create another project 
  that aims to do the same thing as many other projects already did.
  
  Isn't it what humanity is trying to do for thousands of years, 
  since the creation of writing systems?

  And isn't it what in modern era projects like Google, Wikipedia, and in a way even Facebook, are trying to do?

  Yes, organizing the world data is indeed a big mission that many others attacked in many different ways.

  But still, when I try to access the world's information, I find myself sifting through a lot of text, trying to get the truth.

  This project, as most of the projects in the world, might fail. 
  It is very easy for it to fail, for many reasons:
  The mission is too big. 
  My resources are very limited, by time, energy, knowledge and money.
  I would like this project to start as outlet for my personal need of answers to my questions, and a place that supports data organizing on my own way.
  As it might grow and meet some minimal level of quality, it might start to incorporate and influence other projects, e.g. send some data to Wikidata and other open projects.
  
  "Our hands are small, I know
  But they're not yours, they are our own" (Jewel)

  I heard once an entrepeneur who said: "We didn't know this project would succeed, but we knew it must be created."

  So, let's discuss data for a moment.
  Why would anyone need another source of data?
  We already have plenty of books.
  You want to know where is Fiji on the globe?

  The old way: 
  open an encyclopedia or atlas, open the page about Fiji and get to see the location, 
  either by a map, relative to other places etc.

  The new way: 
  open google, type Fiji and see the location in one of the search results

  The problem with the old way: it requires access to a physical object that you might not have available. 
  The problem with the new way: you don't always know what is the source of the information. 
  You might know that you read Wikipedia, but that Wikipedia article might be wrong, since Wikipedia can be edited by anyone.
  Wikipedia should be based on publicly available sources, but it isn't really the case, and even if you have footnote, you not always have access to that book/article to validate.

  Not long ago, a new way was added to this list:
  ChatGPT. You can type your question and the algorithm will answer you something.
  My main problem with this new way: the answer isn't necessarily true. These algorithms are trained to make you believe in the answer, not to get you the right answer.
  Another problem with this way is that you again can't tell what is the source of this answer.

  What can be a solution?
  If sources are the way to validate things in our modern world, let's start from the sources.
  We don't need to have answer to everything, like ChatGPT and google are trying to do.
  But the small amount of data that we do have, we should always have a simple way to know its source.

  We can start with our question about Fiji's location. 
  As ChatGPT told me regarding its source, "Fiji's location in the South Pacific Ocean is a widely known fact and can be confirmed through various reputable sources such as atlases, geographic databases, and official government websites."

  One might say, if Atlases are a valid source for Fiji's location, we can just scan all the Atlases and upload them to the internet.
  First, some projects already do that. But the fact that you can get a PDF copy of an Atlas isn't very accessible way to answer the question about Fiji. 
  You would have to dig in the whole thing/its index in order to find Fiji on the map.
  Second, most sources are still protected by copyright laws.
  All sources contain some amount of text/design that is considered creation and is protected by law for about 70 years after the death of the author.
  
  On Web1.0, some organizations published some information on their websites (e.g. government websites, companies, encyclopedias etc.), 
  so you could access the official site of Fiji, or some online encyclopedia published by Britanica et al.

  One big progress withing Web1.0 was the creation of search engines. 
  You no longer had to know the address of the website you want to get the information from, 
  you just google and get plenty of websites with the text relevent to the search.

  On Web2.0, the web became more croud-sourced, with projects like Wikipedia allowing everyone in the world to share what they think about different subjects, 
  with some guidelines requiring some types of references, with quality varying a lot between languages, articles etc. 
  You can easily find unsupported claims across the platform, some of them are simply wrong.

  One of my previous projects was focused on fixing typos across Wikipedia. 
  My code found over 100,000 typos that have been fixed by volunteers.
  One thing that bothered me about the project: it fixed the typos, but it couldn't fix the factual mistakes. 
  In some cases, it even made it harder to find the mistakes, now that the typos are gone.

  The world need a way to get well-based information. I need a way to get well-based information.

  Another limitation of textual content is that it is not structured. 
  As a programmer, the difference between text and structured data is clear.
  Text you can either serve to the user as is, or make some limited manipulations using parsing.
  Data you can query, filter, join, compare, calculate, order, etc.

  I believe that structured data is key to a well-based and accessible information source.
  All modern organizations have some systems to manage their important data in a structured way. 
  Modern organizations no longer base on papers, or even digitized text, in order to keep track of their business.
  They use software with databases of different kinds powering them,
  allowing fast access to different types of information crucial for the organization's success.

  If at work I am answering my organizational questions using structured data and debuggable software to access it, 
  why can't I use similar tools when I have general-knowledge question?

  In fact, I can. There are already some projects to maintain publicly available databases of general knowledge.
  For example: Wikidata. I can programmatically query or just read the location of Fiji: https://www.wikidata.org/wiki/Q712
  I still have some issue with this database, since many of the claims there lack source, or have the source very general (Wikipedia, Integrated Authority File, etc.)

  Additional advantage of structured data: it isn't protected by copyright. 
  The copyright law protects creative works, the phrasing, the design, not the facts and claims that the work contains.

  One of my previous projects was focused on Wikidata: I imported to Wikidata thousands of words in Latin and Hebrew, with their grammatical information.
  What allowed me to import that big amount of data was the fact that the data was already sructured in the first place, I just had to organize it in the format that Wikidata accepts.

  For a while, I had plans to continue contributing to Wikidata. I truly believe this project's potential. I might as well get back to it. 
  But for now I prefer to explore the option of creating something on my own, with more freedom to experiment.

  Another subject that is worth mentioning is the fact that data is not always accessible to the human reader. 
  For that reason, the project Abstract Wikipedia was formed. When I last checked it was still under construction. 
  I even read they had some fights about the method of work. Some of the staff said it should be template based.
  Others wanted new programming language for the matter.
  I believe in simplicity and in my eyes, templates should work.

  This whole project sounded like endless amount of work. I hope generative AI would help me automate at least part of the process.

</pre>